import json
import json
import os
import queue
import pandas as pd
from  tqdm import tqdm
import ujson

class JSONLReader:

  def __init__(self, folder_path):
    file_path = None
    for fname in os.listdir(folder_path):
      if fname.endswith("valid.jsonl"):
        file_path = os.path.join(folder_path, fname)
        break
    if file_path is None:
      raise FileNotFoundError("No file ending with 'valid.jsonl' found in the folder.")
    self.file_path = file_path

  def read_lines(self, start=None, end=-1):
    """
    Read lines from the JSONL file and return them as a list of dictionaries.

    :param limit: Number of dictionaries to read. If None, read all lines.
    :return: List of dictionaries, each representing a JSON object from the file.
    """
    data = None
    try:
      with open(self.file_path, 'r', encoding='utf-8') as f:
        data = [ujson.loads(line) for line in f]
    except FileNotFoundError:
      print(f"Error: File not found at {self.file_path}")
    except json.JSONDecodeError as e:
      print(f"Error decoding JSON: {e}")
    if start is not None:
      return data[start:end]
    return data

  def write_lines(self, data):
    try:
      with open(self.file_path, 'w', encoding='utf-8') as file:
        for item in data:
          file.write(json.dumps(item, ensure_ascii=False) + '\n')
    except Exception as e:
      print(f"Error writing to file: {e}")

  def filter_lines(self, condition):
    """
    Filter lines in the JSONL file based on a condition.

    :param condition: A function that takes a dictionary and returns True if it should be included.
    :return: List of dictionaries that satisfy the condition.
    """
    data = self.read_lines()
    return [item for item in data if condition(item)]

  def count_lines(self):
    """
    Count the number of lines (JSON objects) in the JSONL file.

    :return: Number of lines in the file.
    """
    return len(self.read_lines())
  
def save_results(results_queue, model_name, dataset_name, output_dir="Results"):
    """
    Save results from the queue to a JSON file incrementally.
    Uses a consistent filename and appends new results to the existing file.
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Generate consistent filename (without timestamp)
    filename = f"{model_name}_{dataset_name}.json"
    filepath = os.path.join(output_dir, filename)
    
    # Collect all results from the queue
    results = []
    while True:
        try:
            item = results_queue.get_nowait()
            if item is None:  # Our signal to stop
                break
            results.append(item)
        except queue.Empty:
            break
    
    # Append to JSON file if we have results
    if results:
        existing_data = []
        # Read existing data if file exists
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            except (json.JSONDecodeError, IOError) as e:
                print(f"Warning: Could not read existing file ({e}), starting fresh")
        
        # Combine old and new data
        combined_data = existing_data + results
        
        # Write back to file
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(combined_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nAdded {len(results)} results to {filepath} (now {len(combined_data)} total)")
    return len(results)


def pr_quintuple_to_string(pr2context):
    """
    å°†PRä¸Šä¸‹æ–‡ä¿¡æ¯è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
    
    Args:
        pr2context: åŒ…å«'5-tuple'å’Œ'node_attribute'çš„å­—å…¸
    
    Returns:
        str: æ ¼å¼åŒ–çš„å­—ç¬¦ä¸²
    """
    lines = []
    if pr2context is None:
      return ""
    
    # å¤„ç†5-tupleä¿¡æ¯
    if '5-tuple' in pr2context:
        lines.append("\nğŸ“Š 5-TUPLE RELATIONSHIPS:")
        lines.append("-" * 40)
        tuples = pr2context['5-tuple']
        lines.append(f"Total tuples: {len(tuples)}")
        
        for i, tuple_data in enumerate(tuples):
            lines.append(f"\nTuple {i+1}:")
            lines.append(f"  Subject: {tuple_data.get('subject', 'N/A')}")
            lines.append(f"  Mention Type: {tuple_data.get('mention-type', 'N/A')}")
            lines.append(f"  Object: {tuple_data.get('object', 'N/A')}")
            lines.append(f"  Edge Attributes: {tuple_data.get('edge_attributes', 'N/A')}")


import networkx as nx
def save_to_mongodb(nodeId_graph_dict, df_stats, edge_connector, prInfo_connector):
    """
    å°†nodeIdå¯¹åº”çš„å›¾ä¿¡æ¯å’Œç»Ÿè®¡æ•°æ®ä¿å­˜åˆ°MongoDB
    
    Args:
        nodeId_graph_dict: èŠ‚ç‚¹IDåˆ°å›¾çš„å­—å…¸
        df_stats: ç»Ÿè®¡ä¿¡æ¯DataFrame
        edge_connector: MongoDBè¾¹è¿æ¥å™¨
        prInfo_connector: MongoDB PRä¿¡æ¯è¿æ¥å™¨
    """
    # è·å–é›†åˆ
    edge_collection = edge_connector.get_collection('nodeId2GraphAndSTAs')
    prInfo_collection = prInfo_connector.get_collection('nodeId2GraphAndSTAs')
    
    print("æ­£åœ¨ä¿å­˜æ•°æ®åˆ°MongoDB...")
    
    # å°†df_statsè½¬æ¢ä¸ºå­—å…¸ï¼Œä¾¿äºæŸ¥æ‰¾
    stats_dict = df_stats.set_index('node_id').to_dict('index')
    
    documents_to_insert = []
    
    for node_id, graph in tqdm(nodeId_graph_dict.items(), desc="Preparing MongoDB documents"):
        # è·å–å¯¹åº”çš„ç»Ÿè®¡ä¿¡æ¯
        stats = stats_dict.get(node_id, {})
        
        # å°†NetworkXå›¾è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        graph_data = {
            'nodes': list(graph.nodes()),
            'edges': list(graph.edges()),
            'node_attributes': {str(node): graph.nodes[node] for node in graph.nodes()},
            'edge_attributes': {f"{str(u)}-{str(v)}": graph.edges[u, v] for u, v in graph.edges()}
        }
        
        # æ„å»ºæ–‡æ¡£
        document = {
            'node_id': node_id,
            'graph_data': graph_data,
            'statistics': stats,
            'timestamp': pd.Timestamp.now(),
            'graph_summary': {
                'num_nodes': graph.number_of_nodes(),
                'num_edges': graph.number_of_edges(),
                'is_connected': nx.is_connected(graph) if graph.number_of_nodes() > 0 else False
            }
        }
        
        documents_to_insert.append(document)
    
    # æ‰¹é‡æ’å…¥åˆ°ä¸¤ä¸ªè¿æ¥å™¨çš„é›†åˆä¸­
    from pymongo.errors import BulkWriteError

    if documents_to_insert:
        # æ’å…¥åˆ°edge_connector
        try:
            result = edge_collection.insert_many(documents_to_insert, ordered=False)
            print(f"æˆåŠŸå‘edge_connectorçš„nodeId2GraphAndSTAsé›†åˆæ’å…¥äº† {len(result.inserted_ids)} æ¡è®°å½•")
        except BulkWriteError as bwe:
            # è·å–æˆåŠŸæ’å…¥çš„æ•°é‡
            inserted_count = bwe.details.get('nInserted', 0)
            write_errors = bwe.details.get('writeErrors', [])
            print(f"edge_connectoræ’å…¥å®Œæˆ: æˆåŠŸ {inserted_count} æ¡ï¼Œå¤±è´¥ {len(write_errors)} æ¡")
            
            # å¯é€‰ï¼šæ‰“å°éƒ¨åˆ†é”™è¯¯ä¿¡æ¯ï¼ˆé¿å…è¾“å‡ºè¿‡å¤šï¼‰
            if write_errors:
                print(f"edge_connectoré”™è¯¯ç¤ºä¾‹: {write_errors[0].get('errmsg', 'æœªçŸ¥é”™è¯¯')}")
        except Exception as e:
            print(f"å‘edge_connectoræ’å…¥æ•°æ®æ—¶å‡ºç°æœªé¢„æœŸé”™è¯¯: {e}")
        
        # æ’å…¥åˆ°prInfo_connector
        try:
            result = prInfo_collection.insert_many(documents_to_insert, ordered=False)
            print(f"æˆåŠŸå‘prInfo_connectorçš„nodeId2GraphAndSTAsé›†åˆæ’å…¥äº† {len(result.inserted_ids)} æ¡è®°å½•")
        except BulkWriteError as bwe:
            # è·å–æˆåŠŸæ’å…¥çš„æ•°é‡
            inserted_count = bwe.details.get('nInserted', 0)
            write_errors = bwe.details.get('writeErrors', [])
            print(f"prInfo_connectoræ’å…¥å®Œæˆ: æˆåŠŸ {inserted_count} æ¡ï¼Œå¤±è´¥ {len(write_errors)} æ¡")
            
            # å¯é€‰ï¼šæ‰“å°éƒ¨åˆ†é”™è¯¯ä¿¡æ¯
            if write_errors:
                print(f"prInfo_connectoré”™è¯¯ç¤ºä¾‹: {write_errors[0].get('errmsg', 'æœªçŸ¥é”™è¯¯')}")
        except Exception as e:
            print(f"å‘prInfo_connectoræ’å…¥æ•°æ®æ—¶å‡ºç°æœªé¢„æœŸé”™è¯¯: {e}")



def save_to_pkl(nodeId_graph_dict, df_stats, output_path="documents_to_insert.pkl"):
    """
    å°†nodeIdå¯¹åº”çš„å›¾ä¿¡æ¯å’Œç»Ÿè®¡æ•°æ®å¤„ç†æˆdocuments_to_insertæ ¼å¼å¹¶ä¿å­˜ä¸ºpklæ–‡ä»¶
    
    Args:
        nodeId_graph_dict: èŠ‚ç‚¹IDåˆ°å›¾çš„å­—å…¸
        df_stats: ç»Ÿè®¡ä¿¡æ¯DataFrame
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º"documents_to_insert.pkl"
    """
    import pickle
    import pandas as pd
    import networkx as nx
    from tqdm import tqdm
    
    print("æ­£åœ¨ä¿å­˜æ•°æ®åˆ°pklæ–‡ä»¶...")
    
    # å°†df_statsè½¬æ¢ä¸ºå­—å…¸ï¼Œä¾¿äºæŸ¥æ‰¾
    stats_dict = df_stats.set_index('node_id').to_dict('index')
    
    documents_to_insert = []
    
    for node_id, graph in tqdm(nodeId_graph_dict.items(), desc="Preparing documents"):
        # è·å–å¯¹åº”çš„ç»Ÿè®¡ä¿¡æ¯
        stats = stats_dict.get(node_id, {})
        
        # å°†NetworkXå›¾è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        graph_data = {
            'nodes': list(graph.nodes()),
            'edges': list(graph.edges()),
            'node_attributes': {str(node): graph.nodes[node] for node in graph.nodes()},
            'edge_attributes': {f"{str(u)}-{str(v)}": graph.edges[u, v] for u, v in graph.edges()}
        }
        
        # æ„å»ºæ–‡æ¡£
        document = {
            'node_id': node_id,
            'graph_data': graph_data,
            'statistics': stats,
            'timestamp': pd.Timestamp.now(),
            'graph_summary': {
                'num_nodes': graph.number_of_nodes(),
                'num_edges': graph.number_of_edges(),
                'is_connected': nx.is_connected(graph) if graph.number_of_nodes() > 0 else False
            }
        }
        
        documents_to_insert.append(document)
    
    # ä¿å­˜documents_to_insertåˆ°pklæ–‡ä»¶
    try:
        with open(output_path, 'wb') as f:
            pickle.dump(documents_to_insert, f)
        print(f"æˆåŠŸä¿å­˜ {len(documents_to_insert)} æ¡æ–‡æ¡£åˆ°: {output_path}")
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")


def load_from_pkl(pkl_path="documents_to_insert.pkl"):
    """
    ä»pklæ–‡ä»¶åŠ è½½documents_to_insertæ•°æ®
    
    Args:
        pkl_path: pklæ–‡ä»¶è·¯å¾„
    
    Returns:
        list: documents_to_insertåˆ—è¡¨
    """
    import pickle
    
    try:
        with open(pkl_path, 'rb') as f:
            documents_to_insert = pickle.load(f)
        print(f"æˆåŠŸä» {pkl_path} åŠ è½½äº† {len(documents_to_insert)} æ¡æ–‡æ¡£")
        return documents_to_insert
    except Exception as e:
        print(f"åŠ è½½pklæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return []