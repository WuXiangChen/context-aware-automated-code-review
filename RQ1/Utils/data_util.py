import json
import os
import pdb
import queue
import ujson
import pickle
import random
from pathlib import Path
from typing import List, Tuple
from Database.node import CodeReviewItem
from tqdm import tqdm
import logging

logger = logging.getLogger("Output/logs/"+__name__)
class JSONLReader:
  def __init__(self, folder_path):
    file_path = None
    for fname in os.listdir(folder_path):
      if fname.endswith("valid.jsonl"):
        file_path = os.path.join(folder_path, fname)
        break
    if file_path is None:
      raise FileNotFoundError("No file ending with 'valid.jsonl' found in the folder.")
    self.file_path = file_path

  def read_lines(self, start=None, end=-1):
    """
    Read lines from the JSONL file and return them as a list of dictionaries.

    :param limit: Number of dictionaries to read. If None, read all lines.
    :return: List of dictionaries, each representing a JSON object from the file.
    """
    data = None
    try:
      with open(self.file_path, 'r', encoding='utf-8') as f:
        data = [ujson.loads(line) for line in f]
    except FileNotFoundError:
      print(f"Error: File not found at {self.file_path}")
    except json.JSONDecodeError as e:
      print(f"Error decoding JSON: {e}")
    if start is not None:
      return data[start:end]
    return data

  def write_lines(self, data):
    try:
      with open(self.file_path, 'w', encoding='utf-8') as file:
        for item in data:
          file.write(json.dumps(item, ensure_ascii=False) + '\n')
    except Exception as e:
      print(f"Error writing to file: {e}")

  def filter_lines(self, condition):
    """
    Filter lines in the JSONL file based on a condition.

    :param condition: A function that takes a dictionary and returns True if it should be included.
    :return: List of dictionaries that satisfy the condition.
    """
    data = self.read_lines()
    return [item for item in data if condition(item)]

  def count_lines(self):
    """
    Count the number of lines (JSON objects) in the JSONL file.

    :return: Number of lines in the file.
    """
    return len(self.read_lines())
  
def save_results(results_queue, model_name, dataset_name, output_dir="Results"):
    """
    Save results from the queue to a JSON file incrementally.
    Uses a consistent filename and appends new results to the existing file.
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_dir, exist_ok=True)
    
    # Generate consistent filename (without timestamp)
    filename = f"{model_name}_{dataset_name}.json"
    filepath = os.path.join(output_dir, filename)
    
    # Collect all results from the queue
    results = []
    while True:
        try:
            item = results_queue.get_nowait()
            if item is None:  # Our signal to stop
                break
            results.append(item)
        except queue.Empty:
            break
    
    # Append to JSON file if we have results
    if results:
        existing_data = []
        # Read existing data if file exists
        if os.path.exists(filepath):
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            except (json.JSONDecodeError, IOError) as e:
                print(f"Warning: Could not read existing file ({e}), starting fresh")
        
        # Combine old and new data
        combined_data = existing_data + results
        
        # Write back to file
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(combined_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nAdded {len(results)} results to {filepath} (now {len(combined_data)} total)")
    return len(results)


def split_and_save_dataset(
    data: List[CodeReviewItem],
    train_ratio: float = 0.8,
    test_ratio: float = 0.2,
    train_file: str = "train_dataset.pkl",
    test_file: str = "test_dataset.pkl",
    random_seed: int = 42,
    shuffle: bool = True,
    output_dir: str = "./Data/"
) -> Tuple[int, int]:
    
    # è¾“å…¥éªŒè¯
    if not data:
        raise ValueError("è¾“å…¥æ•°æ®åˆ—è¡¨ä¸èƒ½ä¸ºç©º")
    
    if abs(train_ratio + test_ratio - 1.0) > 1e-6:
        raise ValueError(f"è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º{train_ratio + test_ratio}")
    
    if train_ratio <= 0 or test_ratio <= 0:
        raise ValueError("è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ¯”ä¾‹å¿…é¡»å¤§äº0")
    
    # è®¾ç½®éšæœºç§å­
    if random_seed is not None:
        random.seed(random_seed)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # æ‰“ä¹±æ•°æ®
    if shuffle:
        random.shuffle(data)
    
    # è®¡ç®—åˆ’åˆ†ç‚¹
    total_size = len(data)
    train_size = int(total_size * train_ratio)
    
    # åˆ’åˆ†æ•°æ®é›†
    train_dataset = data[:train_size]
    test_dataset = data[train_size:]
    
    # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
    train_path = output_path / train_file
    test_path = output_path / test_file
    
    try:
        # ä¿å­˜è®­ç»ƒé›†
        with open(train_path, 'wb') as f:
            pickle.dump(train_dataset, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        # ä¿å­˜æµ‹è¯•é›†
        with open(test_path, 'wb') as f:
            pickle.dump(test_dataset, f, protocol=pickle.HIGHEST_PROTOCOL)
            
        # æ—¥å¿—è®°å½•
        logging.info(f"æ•°æ®é›†åˆ’åˆ†å®Œæˆ:")
        logging.info(f"  æ€»æ•°æ®é‡: {total_size}")
        logging.info(f"  è®­ç»ƒé›†: {len(train_dataset)} æ¡ ({len(train_dataset)/total_size*100:.1f}%)")
        logging.info(f"  æµ‹è¯•é›†: {len(test_dataset)} æ¡ ({len(test_dataset)/total_size*100:.1f}%)")
        logging.info(f"  è®­ç»ƒé›†ä¿å­˜è‡³: {train_path}")
        logging.info(f"  æµ‹è¯•é›†ä¿å­˜è‡³: {test_path}")
        
        print(f"âœ… æ•°æ®é›†åˆ’åˆ†å¹¶ä¿å­˜æˆåŠŸ!")
        print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_dataset)} æ¡æ•°æ® -> {train_path}")
        print(f"ğŸ“Š æµ‹è¯•é›†: {len(test_dataset)} æ¡æ•°æ® -> {test_path}")
        
        return len(train_dataset), len(test_dataset)
        
    except Exception as e:
        raise IOError(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")


def load_dataset(file_path: str) -> List[CodeReviewItem]:
    """
    ä»PKLæ–‡ä»¶åŠ è½½æ•°æ®é›†
    
    Args:
        file_path: PKLæ–‡ä»¶è·¯å¾„
        
    Returns:
        List[CodeReviewItem]: åŠ è½½çš„æ•°æ®é›†
    """
    try:
        with open(file_path, 'rb') as f:
            dataset = pickle.load(f)
        
        print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®é›†: {file_path}")
        print(f"ğŸ“Š æ•°æ®é‡: {len(dataset)} æ¡")
        
        return dataset
        
    except Exception as e:
        raise IOError(f"åŠ è½½æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")


def get_or_load_ref_nodes(data_connector, cache_path="/root/workspace/Context_Aware_ACR_Model/Data/ref_nodes.pkl", load_file=""):
    """
    è·å–ä»£ç è¯„å®¡å‚è€ƒèŠ‚ç‚¹ï¼Œå¸¦æœ‰ç¼“å­˜åŠŸèƒ½
    å¦‚æœç¼“å­˜æ–‡ä»¶å­˜åœ¨åˆ™ç›´æ¥åŠ è½½ï¼Œå¦åˆ™æŸ¥è¯¢æ•°æ®åº“å¹¶ä¿å­˜ç¼“å­˜
    
    Args:
        data_connector: æ•°æ®åº“è¿æ¥å™¨å¯¹è±¡
        cache_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º'ref_nodes.pkl'
    
    Returns:
        List[CodeReview]: ä»£ç è¯„å®¡èŠ‚ç‚¹åˆ—è¡¨
    """
    ref_node_list = None
    cache_file = Path(load_file)
    if cache_file.exists():
        ref_node_list = load_dataset(load_file)
        return ref_node_list
    
    # ç¼“å­˜ä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥ï¼Œä»æ•°æ®åº“æŸ¥è¯¢
    if ref_node_list is None:
      cache_file = Path(cache_path)
      print("Querying ref nodes from database...")
      graph_base_path = "/root/workspace/Context_Aware_ACR_Model/Data/merged.pkl" # è¿™ä¸ªä¸œè¥¿æ˜¯ä¸€å®šå­˜åœ¨çš„ï¼
      cache_file = Path("/root/workspace/Context_Aware_ACR_Model/Data/ref_nodes.pkl") # ä¿å­˜å¯¹è±¡
      if not cache_file.exists():
        with open(graph_base_path, 'rb') as f:
            graph_dict = pickle.load(f) # è¿™é‡ŒåŠ è½½çš„graphæ•°æ®ä¸€å®šæ˜¯Database/nodeInfoForContextGen.pyæ–‡ä»¶ä¸­å®šä¹‰çš„NodeCInfo
        # è¿™é‡Œåªè·å–æ‰€æœ‰çš„REFæ•°æ®
        ref_node_list = data_connector.get_only_CodeReviewerDatasetREF(graph_dict)
        del graph_dict
        # ç„¶åå°†è·å–è€Œå¾—çš„REFæ•°æ®ä¸è¿™é‡Œçš„Data/merged.pklæ•°æ®è¿›è¡Œæ‹¼æ¥
        
        # ä¿å­˜æŸ¥è¯¢ç»“æœåˆ°ç¼“å­˜
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(ref_node_list, f)
            print(f"Saved ref nodes to cache: {cache_path}")
        except Exception as e:
            print(f"Warning: Failed to save cache. Error: {e}")

        ### å»é™¤æ‰€æœ‰çš„æ— å‰é©±èŠ‚ç‚¹
        ref_node_list_filtered = [item for item in ref_node_list if item.graph_summary.num_nodes!=0]
        split_and_save_dataset(ref_node_list_filtered)
        return load_dataset(load_file)

import gc
import sys
from pympler import asizeof, muppy, summary
from AllocMethod import *
def get_node_list_with_cache(args, data_connector):
    """åŠ è½½æˆ–ç”Ÿæˆå¸¦ç¼“å­˜çš„node_list"""
    # ç¼“å­˜æ–‡ä»¶è·¯å¾„
    cache_file = Path(f"/root/workspace/Context_Aware_ACR_Model/Data/alloc{args.allocMethod}Context_NodeList_{'Training' if args.train_eval else 'Test'}.pkl")
    
    # å°è¯•åŠ è½½ç¼“å­˜
    if cache_file.exists():
        try:
            with open(cache_file, 'rb') as f:
                node_list = pickle.load(f)
            logger.info(f"ä»ç¼“å­˜åŠ è½½ {len(node_list)} ä¸ªèŠ‚ç‚¹")
            if args.allocMethod == "FullSampling15":
                node_list = [item for node in node_list for item in node.fsContextItems ]
            return node_list
        except Exception as e:
            logger.warning(f"ç¼“å­˜åŠ è½½å¤±è´¥: {e}ï¼Œé‡æ–°ç”Ÿæˆ")
    
    # ç”Ÿæˆæ–°æ•°æ®
    logger.info("ç”Ÿæˆæ–°çš„node_list...")
    if args.train_eval:
        load_file = "/root/workspace/Context_Aware_ACR_Model/Data/train_dataset.pkl"
    else:        
        load_file = "/root/workspace/Context_Aware_ACR_Model/Data/test_dataset.pkl"
    
    ref_node_list = get_or_load_ref_nodes(data_connector, load_file=load_file)
    context_func = eval(f"alloc{args.allocMethod}Context")
    
    if not args.allocMethod.startswith("FullSampling"):
        node_list = [context_func(acrItem=node) for node in tqdm(ref_node_list, desc="Processing nodes")]
    else:
        node_list = [
            item 
            for node in tqdm(ref_node_list, desc="Processing FullSampling nodes") 
            for item in context_func(acrItem=node).fsContextItems  # å‡è®¾fsContextItemsæ˜¯å¯è¿­ä»£å¯¹è±¡
        ]
    # ä¿å­˜ç¼“å­˜
    cache_file.parent.mkdir(parents=True, exist_ok=True)
    with open(cache_file, 'wb') as f:
        pickle.dump(node_list, f)
    
    # æ¸…ç†å†…å­˜
    data_connector.disconnect()
    del ref_node_list
    
    total_size = asizeof.asizeof(node_list) / (1024 ** 3)
    collected = gc.collect()
    logger.info(f"ç”Ÿæˆå®Œæˆ: {len(node_list)} ä¸ªèŠ‚ç‚¹, {total_size:.2f} GB, å›æ”¶ {collected} ä¸ªå¯¹è±¡")
    
    return node_list
