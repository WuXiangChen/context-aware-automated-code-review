import hashlib
import logging
import time
import torch
import torch.optim as optim
import numpy as np
import torch.nn.functional as F
from Database.evaluationCache import EvaluationCache
from Model import *
from AllocMethod import *
from Utils.rl_util import evaluate_single_example, generate_context_from_nodeList, get_simple_model_signature, preprocess_graph_data

logger = logging.getLogger("Output/logs/"+__name__)
# stateå®šä¹‰ä¸ºä»€ä¹ˆï¼Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯å·²é€‰èŠ‚ç‚¹é›†åˆçš„context embedding
# criticçš„è¾“å…¥æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯context embeddingï¼Œcodediff embedding å’Œ å¯¹åº”æŸä¸ªstateçš„actionï¼›è¿™è¡¨æ˜æ¨¡å‹åœ¨å­¦ä¹ çš„æ˜¯å½“å‰(s,a)çŠ¶æ€-åŠ¨ä½œå¯¹çš„Qå‡½æ•°
# actorçš„è¾“å…¥æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿ å¾ˆç®€å•ï¼Œå°±æ˜¯å½“å‰å›¾çš„èŠ‚ç‚¹ç‰¹å¾ï¼ˆå›¾ä¸Šæ¯ä¸ªèŠ‚ç‚¹çš„èŠ‚ç‚¹ç‰¹å¾ï¼Œå°±å®šä¹‰ä¸ºå…¶è‡ªç„¶æ–‡æœ¬æ‹¼æ¥çš„embeddingå€¼ï¼‰ä»¥åŠè¾¹é›†åˆã€‚
class SimplifiedRLTrainer:
    """ç®€åŒ–çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨ - é…åˆé¢„è®¾çš„Actor-Criticæ¶æ„"""
    def __init__(self, args, actor, critic, predefined_bleu_model, rfDataTrans, pool, predefined_args):
        self.args = args
        # self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.device = torch.device('cuda:0')
        
        # ä½¿ç”¨ä¼ å…¥çš„Actorå’ŒCriticç½‘ç»œ
        self.actor = actor.to(self.device)
        self.critic = critic.to(self.device)
        self.predefined_bleu_model = predefined_bleu_model.to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=args.actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=args.critic_lr)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.actor_losses = []
        self.critic_losses = []
        
        self.ec = EvaluationCache()
        self.SAVE_COUNTER = 0
        
        self.rfDataTrans = rfDataTrans
        self.pool = pool
        self.predefined_args = predefined_args
    
    def flush_ec(self):
        self.ec.save_cache()
        self.ec = EvaluationCache()
     
    def calculate_final_reward(self, current_nodes, idx_to_node, graph_data, crItem):
        """
        è®¡ç®—æœ€ç»ˆå¥–åŠ±çš„æˆå‘˜æ–¹æ³•
        
        Args:
            current_nodes: å½“å‰èŠ‚ç‚¹åˆ—è¡¨
            idx_to_node: ç´¢å¼•åˆ°èŠ‚ç‚¹çš„æ˜ å°„
            graph_data: å›¾æ•°æ®
            crItem: å½“å‰é¡¹ç›®å­—å…¸
            predefined_args: é¢„å®šä¹‰å‚æ•°
        
        Returns:
            final_reward: è®¡ç®—å¾—åˆ°çš„æœ€ç»ˆå¥–åŠ±å€¼
        """
        # TODO 2: å®ç°è¿™é‡Œè‡ªç„¶æ‰©å±•ä¸Šä¸‹æ–‡ï¼Œå¹¶å¯¹åº”ç”Ÿæˆembeddingçš„æ–¹æ³• âœ“
        node_ids = [idx_to_node[item] for item in current_nodes]
        crItem["context"] = generate_context_from_nodeList(graph_data, node_ids)
        state = self.rfDataTrans.tokenize(item=(crItem, self.rfDataTrans.tokenizer, self.predefined_args))
        
        # TODO 3: å®ç°è¿™é‡Œå°†embeddingä½œä¸ºè¾“å…¥ï¼Œå¹¶å¯¹åº”ç”Ÿæˆæ–‡æœ¬è¾“å‡ºä¸bleu rewardçš„æ–¹æ³•
        model_info = ""
        if hasattr(self.predefined_bleu_model, 'config') and hasattr(self.predefined_bleu_model.config, 'vocab_size'):
            model_info += get_simple_model_signature(
                self.predefined_bleu_model, 
                beam_size=self.predefined_args.beam_size, 
                tokenizer=self.rfDataTrans.tokenizer, 
                args=self.predefined_args
            )

        # ç”Ÿæˆç¼“å­˜é”®
        combined_input = f"{crItem['context']}###MODEL###{model_info}"
        sha_context = hashlib.sha256(combined_input.encode()).hexdigest()
        
        # æ£€æŸ¥ç¼“å­˜
        if sha_context in self.ec.cache:
            print(f"Cache hit for SHA: {sha_context[:8]}...")
            final_reward = self.ec.cache[sha_context]
        else:
            print(f"Cache miss for SHA: {sha_context[:8]}...")
            # æ‰§è¡Œè¯„ä¼°
            final_reward = evaluate_single_example(
                model=self.predefined_bleu_model, 
                example=state, 
                device=self.device, 
                beam_size=self.predefined_args.beam_size, 
                tokenizer=self.rfDataTrans.tokenizer
            )
            # å­˜å‚¨åˆ°ç¼“å­˜
            self.ec.cache[sha_context] = final_reward
            self._increment_save_counter()
            print(f"ğŸ’¾ New evaluation: {final_reward}")
        
        return final_reward

    def _increment_save_counter(self):
        """
        å¢åŠ ä¿å­˜è®¡æ•°å™¨å¹¶åœ¨å¿…è¦æ—¶åˆ·æ–°ç¼“å­˜
        """
        self.SAVE_COUNTER += 1
        if self.SAVE_COUNTER % 1000 == 0:  # æ¯5æ¬¡ä¿å­˜
            self.flush_ec()
    
    def collect_episode(self, crItem, state):
        """æ”¶é›†ä¸€ä¸ªå®Œæ•´çš„episode"""
        graph_data = crItem["graph_data"]
        graph_data["node_attributes"]["features"] = []
        node_to_idx = {node: idx for idx, node in enumerate(graph_data["nodes"])}
        idx_to_node = {idx: node for node, idx in node_to_idx.items()}
        
        for node_id in graph_data["nodes"]:
            node_context = generate_context_from_nodeList(graph_data, [node_id])
            feature = self.rfDataTrans.encode_remove(self.rfDataTrans.tokenizer, node_context, args=self.predefined_args)
            feature, _ = self.rfDataTrans.pad_assert(feature, [], args=self.predefined_args, tokenizer=self.rfDataTrans.tokenizer)
            graph_data["node_attributes"]["features"].append(feature)
                
        node_features, edge_index = preprocess_graph_data(graph_data=graph_data, device=self.device, core_id=core_id, node_to_idx=node_to_idx)
        
        
        core_id = "PR-"+str(crItem["ghid"])
        trajectory = []
        current_nodes=[node_to_idx[core_id]]
        # æ”¹æˆå¤šå›¾å¹¶è¡Œçš„æ‰§è¡Œ
        while True:
            # Actoré‡‡æ ·åŠ¨ä½œï¼Œè¿™é‡Œé‡‡æ ·åŠ¨ä½œçš„è¾“å…¥åº”è¯¥æ˜¯node_features, edge_index, current_nodesï¼Œè¿™é‡Œéœ€è¦è¿›è¡Œè°ƒæ•´
            '''TODO 1: å°†è¿™é‡Œçš„sample_actionçš„è¾“å…¥è°ƒæ•´æ­£ç¡®ï¼Œæœ€å¥½èƒ½ææˆæ ‡å‡†çš„è¾“å…¥æ ¼å¼ âˆš'''
            action, log_prob, entropy, selected_node, is_stop = self.actor.sample_action(node_features, edge_index, current_nodes)
            
            if is_stop:
                # è®¡ç®—æœ€ç»ˆå¥–åŠ±
                final_reward = self.calculate_final_reward(current_nodes, idx_to_node, graph_data, crItem)
                
                # è®°å½•æœ€åä¸€æ­¥
                trajectory.append({
                    'state': torch.tensor(state.source_ids, dtype=torch.float32),
                    'action': action,
                    'log_prob': log_prob,
                    'entropy': entropy,
                    'reward': final_reward,
                    'is_terminal': True
                })
                break
            else:
                # é€‰æ‹©èŠ‚ç‚¹ï¼Œç»§ç»­episode
                selected_node = action.item()
                current_nodes.append(selected_node)
                # ä¸­é—´å¥–åŠ±ï¼ˆå¯ä»¥è®¾ä¸º0æˆ–å°çš„æ­£å¥–åŠ±ï¼‰
                intermediate_reward = self.calculate_final_reward(current_nodes, idx_to_node, graph_data, crItem)
                trajectory.append({
                    'state': torch.tensor(state.source_ids, dtype=torch.float32),
                    'action': action,
                    'log_prob': log_prob,
                    'entropy': entropy,
                    'reward': intermediate_reward,
                    'is_terminal': False
                })
                
            '''TODO 2: æ›´æ–°stateçš„å˜åŒ– âˆš'''
            node_ids = [idx_to_node[item] for item in current_nodes]
            crItem["context"] = generate_context_from_nodeList(graph_data, node_ids)
            state = self.rfDataTrans.tokenize(item=(crItem, self.rfDataTrans.tokenizer, self.predefined_args))
            
        return trajectory
    
    def compute_advantages_for_episodes(self, trajectories, rewards, q_values):
        """ä¸ºepisodeè½¨è¿¹è®¡ç®—advantages"""
        returns = []
        advantages = []
        
        # æŒ‰episodeåˆ†ç»„å¤„ç†
        episode_starts = [0]
        for i, step in enumerate(trajectories):
            if step['is_terminal']:
                episode_starts.append(i + 1)
        
        for start_idx in range(len(episode_starts) - 1):
            episode_start = episode_starts[start_idx]
            episode_end = episode_starts[start_idx + 1]
            
            # è®¡ç®—è¿™ä¸ªepisodeçš„returns
            episode_rewards = rewards[episode_start:episode_end]
            episode_q_values = q_values[episode_start:episode_end]
            
            # ä»åå¾€å‰è®¡ç®—discounted returns
            episode_returns = []
            running_return = 0.0
            
            for r in reversed(episode_rewards):
                running_return = r + self.args.gamma * running_return
                episode_returns.append(running_return)
            
            episode_returns.reverse()
            
            # è®¡ç®—advantages
            episode_advantages = [ret - q_val for ret, q_val in zip(episode_returns, episode_q_values)]
            
            returns.extend(episode_returns)
            advantages.extend(episode_advantages)
        
        return torch.tensor(returns, device=self.device), torch.tensor(advantages, device=self.device)

    def train_step(self, batch_data):
        """å•æ­¥è®­ç»ƒ - å¤„ç†å®Œæ•´episodes"""
        # batch_data å°±æ˜¯ä¸€ä¸ªlist
        state = self.pool.map(self.rfDataTrans.tokenize, [(example, self.rfDataTrans.tokenizer, self.predefined_args) for example in batch_data])
        batch_size = len(batch_data)
        # 1. æ”¶é›†æ‰€æœ‰episodesçš„è½¨è¿¹
        all_trajectories = []
        for i in range(batch_size):
            trajectory = self.collect_episode(
                crItem=batch_data[i],
                state=state[i]
            )
            all_trajectories.extend(trajectory)
        
        # 2. æå–è½¨è¿¹æ•°æ®
        states = []
        actions = []
        log_probs = []
        rewards = []
        entropies = []
        is_terminals = []
        
        for step in all_trajectories:
            states.append(step['state'])
            actions.append(step['action'])
            log_probs.append(step['log_prob'])
            rewards.append(step['reward'])
            entropies.append(step['entropy'])
            is_terminals.append(step['is_terminal'])
        
        # è½¬æ¢ä¸ºtensor
        actions = torch.stack(actions)
        log_probs = torch.stack(log_probs)
        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)
        entropies = torch.stack(entropies)
        
        # 3&4. è®¡ç®—Vå€¼ã€returnså’Œadvantages
        predicted_state_values = []
        state_values = []

        for i, state in enumerate(states):
            v_val = self.critic(state.to(self.device))  # shape: [1, 1]
            predicted_state_values.append(v_val)
            state_values.append(v_val.item())

        # æ‹¼æ¥ä¸º [batch_size, 1]
        predicted_state_values = torch.cat(predicted_state_values, dim=0).squeeze()

        # è®¡ç®— returnsï¼ˆå·²æŠ˜æ‰£çš„ç´¯è®¡å¥–åŠ±ï¼‰å’Œ advantages = returns - V(s)
        returns, _ = self.compute_advantages_for_episodes(all_trajectories, rewards.cpu().numpy(), state_values)
        returns_tensor = returns.clone().detach().to(predicted_state_values.device).float()
        advantages = returns_tensor - predicted_state_values
        advantages = torch.clamp(advantages, -10, 10)
        
        # critic lossï¼šæ‹Ÿåˆ V(s)
        critic_loss = F.smooth_l1_loss(predicted_state_values, returns_tensor)
        # critic_loss = critic_loss * 0.01
        
        # 5. æ›´æ–° critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.args.max_grad_norm)
        self.critic_optimizer.step()

        # 6. æ›´æ–° actor
        actor_loss = -(log_probs * advantages.detach()).mean()
        actor_loss -= self.args.entropy_weight * entropies.mean()

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.args.max_grad_norm)
        self.actor_optimizer.step()

        return {
            'actor_loss': actor_loss.item(),
            'critic_loss': critic_loss.item(),
            'avg_reward': rewards.mean().item(),
            'avg_advantage': advantages.mean().item(),
            'avg_episode_length': len(all_trajectories) / batch_size
        }
   
    def train_epoch(self, dataloader, epoch_num=None):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.actor.train()
        self.critic.train()
        
        epoch_stats = {
            'actor_losses': [],
            'critic_losses': [],
            'rewards': [],
            'advantages': [],
            'episode_lengths': []
        }
        
        total_batches = len(dataloader)
        start_time = time.time()
        
        for batch_idx, batch_data in enumerate(dataloader):
            batch_start_time = time.time()
            
            # è®­ç»ƒä¸€ä¸ªbatch
            stats = self.train_step(batch_data)
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            epoch_stats['actor_losses'].append(stats['actor_loss'])
            epoch_stats['critic_losses'].append(stats['critic_loss'])
            epoch_stats['rewards'].append(stats['avg_reward'])
            epoch_stats['advantages'].append(stats['avg_advantage'])
            epoch_stats['episode_lengths'].append(stats['avg_episode_length'])
            
            batch_time = time.time() - batch_start_time
            
            # å®šæœŸæ‰“å°è¯¦ç»†ä¿¡æ¯
            if batch_idx % self.args.log_interval == 0:
                elapsed_time = time.time() - start_time
                progress = (batch_idx + 1) / total_batches * 100
                
                # è®¡ç®—ETA
                avg_batch_time = elapsed_time / (batch_idx + 1)
                eta_seconds = avg_batch_time * (total_batches - batch_idx - 1)
                eta_str = time.strftime('%H:%M:%S', time.gmtime(eta_seconds))
                
                # è®¡ç®—å½“å‰å­¦ä¹ ç‡
                actor_lr = self.actor_optimizer.param_groups[0]['lr']
                critic_lr = self.critic_optimizer.param_groups[0]['lr']
                
                epoch_prefix = f"Epoch {epoch_num}" if epoch_num is not None else "Training"
                
                logger.info(f"{epoch_prefix} - Batch [{batch_idx+1}/{total_batches}] ({progress:.1f}%)")
                logger.info(f"  â”œâ”€ Actor Loss: {stats['actor_loss']:.6f} | Critic Loss: {stats['critic_loss']:.6f}")
                logger.info(f"  â”œâ”€ Avg Reward: {stats['avg_reward']:.6f} | Avg Advantage: {stats['avg_advantage']:.6f}")
                logger.info(f"  â”œâ”€ Avg Episode Length: {stats['avg_episode_length']:.2f}")
                logger.info(f"  â”œâ”€ Learning Rate - Actor: {actor_lr:.2e} | Critic: {critic_lr:.2e}")
                logger.info(f"  â”œâ”€ Batch Time: {batch_time:.2f}s | Avg Time: {avg_batch_time:.2f}s/batch")
                logger.info(f"  â””â”€ ETA: {eta_str} | Elapsed: {time.strftime('%H:%M:%S', time.gmtime(elapsed_time))}")
                logger.info("")
            
            # æ¯ä¸ªbatchåçš„ç®€å•è¿›åº¦æç¤º
            elif batch_idx % (self.args.log_interval // 4) == 0:  # æ›´é¢‘ç¹çš„ç®€å•è¿›åº¦
                progress = (batch_idx + 1) / total_batches * 100
                logger.info(f"Progress: {progress:.1f}% | Batch {batch_idx+1}/{total_batches} | " f"Reward: {stats['avg_reward']:.4f} | Actor Loss: {stats['actor_loss']:.4f}")
        
        total_time = time.time() - start_time
        
        # è®¡ç®—epochå¹³å‡ç»Ÿè®¡
        avg_stats = {
            'avg_actor_loss': np.mean(epoch_stats['actor_losses']),
            'avg_critic_loss': np.mean(epoch_stats['critic_losses']),
            'avg_reward': np.mean(epoch_stats['rewards']),
            'avg_advantage': np.mean(epoch_stats['advantages']),
            'avg_episode_length': np.mean(epoch_stats['episode_lengths']),
            'std_reward': np.std(epoch_stats['rewards']),
            'max_reward': np.max(epoch_stats['rewards']),
            'min_reward': np.min(epoch_stats['rewards']),
            'total_batches': total_batches,
            'epoch_time': total_time
        }
        
        # æ‰“å°epochæ€»ç»“
        epoch_prefix = f"Epoch {epoch_num}" if epoch_num is not None else "Training Epoch"
        logger.info("=" * 80)
        logger.info(f"{epoch_prefix} Summary:")
        logger.info(f"  â”œâ”€ Total Batches: {total_batches} | Total Time: {time.strftime('%H:%M:%S', time.gmtime(total_time))}")
        logger.info(f"  â”œâ”€ Avg Actor Loss: {avg_stats['avg_actor_loss']:.6f} | Avg Critic Loss: {avg_stats['avg_critic_loss']:.6f}")
        logger.info(f"  â”œâ”€ Avg Reward: {avg_stats['avg_reward']:.6f} Â± {avg_stats['std_reward']:.6f}")
        logger.info(f"  â”œâ”€ Reward Range: [{avg_stats['min_reward']:.6f}, {avg_stats['max_reward']:.6f}]")
        logger.info(f"  â”œâ”€ Avg Advantage: {avg_stats['avg_advantage']:.6f}")
        logger.info(f"  â””â”€ Avg Episode Length: {avg_stats['avg_episode_length']:.2f}")
        logger.info("=" * 80)
        logger.info("")
        
        return avg_stats
    
    def evaluate(self, dataloader):
        """è¯„ä¼°æ¨¡å‹"""
        self.actor.eval()
        self.critic.eval()
        rewards = []
        all_size = 0
        with torch.no_grad():
            # å¤„ç†å•ä¸ªæ ·æœ¬çš„embedding
            for batch_idx, batch_data in enumerate(dataloader):
                state = self.pool.map(self.rfDataTrans.tokenize, [(example, self.rfDataTrans.tokenizer, self.predefined_args) for example in batch_data])
                batch_size = len(batch_data)
                all_size += batch_size
                # æ”¶é›†è½¨è¿¹
                all_trajectories = []
                for i in range(batch_size):
                    trajectory = self.collect_episode(
                        crItem=batch_data[i],
                        state=state[i]
                    )
                    all_trajectories.extend(trajectory)
                for step in all_trajectories:
                    if step['is_terminal']:
                        rewards.append(step['reward']) 
                
        if rewards:  # æ£€æŸ¥æ˜¯å¦æœ‰å¥–åŠ±æ•°æ®
            mean_bleu = np.sum(rewards) / batch_size  # ä¿®æ­£ï¼šé™¤ä»¥batch_sizeè€Œä¸æ˜¯len(batch_size)
        else:
            mean_bleu = 0.0  # å¤„ç†ç©ºå¥–åŠ±åˆ—è¡¨çš„æƒ…å†µ
        self.actor.train()
        self.critic.train()
        return mean_bleu
    
    # è§„åˆ’trainä¸testçš„éƒ¨åˆ†
    def run(self, train_dataloader, val_dataloader=None, num_epochs=100):
        """è¿è¡Œè®­ç»ƒ"""
        logger.info("Starting Simplified RL Training")
        
        best_reward = -float('inf')
        
        for epoch in range(num_epochs):
            # è®­ç»ƒ
            train_stats = self.train_epoch(train_dataloader)
            
            # ä¸€ä¸ªepochå°±æ˜¯ä¸€ä¸ªepisode
            self.episode_rewards.append(train_stats['avg_reward'])
            self.actor_losses.append(train_stats['avg_actor_loss'])
            self.critic_losses.append(train_stats['avg_critic_loss'])
            
            # éªŒè¯
            if val_dataloader is not None:
                val_reward = self.evaluate(val_dataloader)
                logger.info(f"Epoch {epoch}: "
                           f"Train Reward: {train_stats['avg_reward']:.4f}, "
                           f"Val Reward: {val_reward:.4f}, "
                           f"Actor Loss: {train_stats['avg_actor_loss']:.4f}, "
                           f"Critic Loss: {train_stats['avg_critic_loss']:.4f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_reward > best_reward:
                    best_reward = val_reward
                    self.save_models(epoch, is_best=True)
            else:
                logger.info(f"Epoch {epoch}: "
                           f"Train Reward: {train_stats['avg_reward']:.4f}, "
                           f"Actor Loss: {train_stats['avg_actor_loss']:.4f}, "
                           f"Critic Loss: {train_stats['avg_critic_loss']:.4f}")
            
            # å®šæœŸä¿å­˜
            if epoch % self.args.save_interval == 0:
                self.save_models(epoch)
    
    def save_models(self, epoch, is_best=False):
        """ä¿å­˜æ¨¡å‹"""
        suffix = '_best' if is_best else f'_epoch_{epoch}'
        
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'epoch': epoch,
            'episode_rewards': self.episode_rewards,
            'actor_losses': self.actor_losses,
            'critic_losses': self.critic_losses
        }, f'{self.args.output_dir}/rl_model{suffix}.pt')
        logger.info(f"Model saved: rl_model{suffix}.pt")

    def load_models(self, model_path, load_optimizer=True):
        """åŠ è½½æ¨¡å‹"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # åŠ è½½æ¨¡å‹å‚æ•°
            self.actor.load_state_dict(checkpoint['actor_state_dict'])
            self.critic.load_state_dict(checkpoint['critic_state_dict'])
            
            # å¯é€‰æ‹©æ˜¯å¦åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            if load_optimizer:
                self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
                self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
            
            # åŠ è½½è®­ç»ƒå†å²ä¿¡æ¯
            epoch = checkpoint.get('epoch', 0)
            self.episode_rewards = checkpoint.get('episode_rewards', [])
            self.actor_losses = checkpoint.get('actor_losses', [])
            self.critic_losses = checkpoint.get('critic_losses', [])
            
            logger.info(f"Model loaded successfully from {model_path}")
            logger.info(f"Resumed from epoch {epoch}")
            
            return epoch
            
        except FileNotFoundError:
            logger.error(f"Model file not found: {model_path}")
            raise
        except KeyError as e:
            logger.error(f"Missing key in checkpoint: {e}")
            raise
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            raise