from copy import deepcopy
import copy
import hashlib
import logging
import time
import torch
import torch.optim as optim
import numpy as np
import pandas as pd
import torch.nn.functional as F
from Database.evaluationCache import EvaluationCache
from Model.actorCritic.collectEpisodesMerged import collect_episodes_merged
from Model import *
from AllocMethod import *
from Model.actorCritic.trainingVisualizer import TrainingVisualizer
from Model.actorCritic.util import pad_trajectories_to_numpy
from Utils.rl_util import evaluate_batch_examples, generate_context_from_nodeList, get_simple_model_signature, preprocess_graph_data
from typing import Optional
from tqdm import tqdm
logger = logging.getLogger("Output/logs/"+__name__)
# stateå®šä¹‰ä¸ºä»€ä¹ˆï¼Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯å·²é€‰èŠ‚ç‚¹é›†åˆçš„context embedding
# criticçš„è¾“å…¥æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯context embeddingï¼Œcodediff embedding å’Œ å¯¹åº”æŸä¸ªstateçš„actionï¼›è¿™è¡¨æ˜æ¨¡å‹åœ¨å­¦ä¹ çš„æ˜¯å½“å‰(s,a)çŠ¶æ€-åŠ¨ä½œå¯¹çš„Qå‡½æ•°
# actorçš„è¾“å…¥æ¨¡å‹æ˜¯ä»€ä¹ˆï¼Ÿ å¾ˆç®€å•ï¼Œå°±æ˜¯å½“å‰å›¾çš„èŠ‚ç‚¹ç‰¹å¾ï¼ˆå›¾ä¸Šæ¯ä¸ªèŠ‚ç‚¹çš„èŠ‚ç‚¹ç‰¹å¾ï¼Œå°±å®šä¹‰ä¸ºå…¶è‡ªç„¶æ–‡æœ¬æ‹¼æ¥çš„embeddingå€¼ï¼‰ä»¥åŠè¾¹é›†åˆã€‚
class SimplifiedRLTrainer:
    """ç®€åŒ–çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨ - é…åˆé¢„è®¾çš„Actor-Criticæ¶æ„"""
    def __init__(self, args, actor, critic, predefined_bleu_model, rfDataTrans, pool, predefined_args):
        self.args = args
        # self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        # self.device = torch.device(predefined_args.device)
        self.device = torch.device(predefined_args.device)
        print(f"device:{torch.device(predefined_args.device)}")
        
        # ä½¿ç”¨ä¼ å…¥çš„Actorå’ŒCriticç½‘ç»œ
        self.actor = actor.to(self.device)
        self.critic = critic.to(self.device)
        self.predefined_bleu_model = predefined_bleu_model.to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=args.actor_lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=args.critic_lr)
        
        # è®­ç»ƒç»Ÿè®¡
        self.episode_rewards = []
        self.actor_losses = []
        self.critic_losses = []
        
        self.rfDataTrans = rfDataTrans
        self.pool = pool
        self.predefined_args = predefined_args
        
        if self.predefined_args.task_type == "msg":
            self.cached_file_name = f"evaluation_cache_msg_codediff_{predefined_args.maxRL}.json"
        elif self.predefined_args.task_type == "ref":
            self.cached_file_name = "evaluation_cache_ref.json"
        self.ec = EvaluationCache(self.cached_file_name)
        self.SAVE_COUNTER = 0
    
    def flush_ec(self):
        self.ec.save_cache()
        self.ec = EvaluationCache(self.cached_file_name)
        self.SAVE_COUNTER = 0
        
    def get_sha256(self, crItem):
        """
        è®¡ç®—æœ€ç»ˆå¥–åŠ±çš„æˆå‘˜æ–¹æ³•
        
        Args:
            current_nodes: å½“å‰èŠ‚ç‚¹åˆ—è¡¨
            idx_to_node: ç´¢å¼•åˆ°èŠ‚ç‚¹çš„æ˜ å°„
            graph_data: å›¾æ•°æ®
            crItem: å½“å‰é¡¹ç›®å­—å…¸
            predefined_args: é¢„å®šä¹‰å‚æ•°
        
        Returns:
            final_reward: è®¡ç®—å¾—åˆ°çš„æœ€ç»ˆå¥–åŠ±å€¼
        """
        # TODO 3: å®ç°è¿™é‡Œå°†embeddingä½œä¸ºè¾“å…¥ï¼Œå¹¶å¯¹åº”ç”Ÿæˆæ–‡æœ¬è¾“å‡ºä¸bleu rewardçš„æ–¹æ³•
        model_info = ""
        if hasattr(self.predefined_bleu_model, 'config') and hasattr(self.predefined_bleu_model.config, 'vocab_size'):
            model_info += get_simple_model_signature(
                self.predefined_bleu_model, 
                beam_size=self.predefined_args.beam_size, 
                tokenizer=self.rfDataTrans.tokenizer, 
                args=self.predefined_args
            )

        # ç”Ÿæˆç¼“å­˜é”®
        combined_input = f"{crItem['context']}###MODEL###{model_info}"
        sha_context = hashlib.sha256(combined_input.encode()).hexdigest()
        
        return sha_context

    def calculate_bleu_from_cache(self, crItem):
        """
        ä»ç¼“å­˜ä¸­è·å–BLEUåˆ†æ•°
            
        Returns:
            tuple: (æ˜¯å¦å‘½ä¸­ç¼“å­˜, BLEUåˆ†æ•°) 
                å¦‚æœå‘½ä¸­è¿”å› (True, score)
                å¦‚æœæœªå‘½ä¸­è¿”å› (False, None)
        """
        sha_context = self.get_sha256(crItem)
        
        if sha_context in self.ec.cache:
            return self.ec.cache[sha_context]
        else:
            # print(f"Cache miss for SHA: {sha_context[:8]}...")
            return False

    def calculate_bleu_from_preTrainedModel(self, states, crItems, saved_flag=True):
        """
        ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è®¡ç®—BLEUåˆ†æ•°
        
        Args:
            state: å½“å‰çŠ¶æ€
            combined_input: ç”¨äºç”Ÿæˆç¼“å­˜é”®çš„è¾“å…¥å­—ç¬¦ä¸²
            
        Returns:
            float: è®¡ç®—å¾—åˆ°çš„BLEUåˆ†æ•°
        """
        # æ‰¹é‡ç”Ÿæˆç¼“å­˜é”®
        sha_contexts = [self.get_sha256(crItem) for crItem in crItems]
        # æ‰§è¡Œè¯„ä¼°
        final_bleu, all_other_metrics = evaluate_batch_examples(
            model=self.predefined_bleu_model, 
            examples=states, 
            device=self.device, 
            beam_size=self.predefined_args.beam_size, 
            tokenizer=self.rfDataTrans.tokenizer,
            llm_device=self.predefined_args.llm_device
        )
        
        # å­˜å‚¨åˆ°ç¼“å­˜
        if saved_flag:
            for k, sha_context in enumerate(sha_contexts):
                bleu = final_bleu[k]
                all_metrics = all_other_metrics[k]
                all_metrics["bleu"] = bleu
                self.ec.cache[sha_context] = all_metrics
                self._increment_save_counter()
            print(f"ğŸ’¾ New evaluation: {final_bleu}")
            torch.cuda.empty_cache()
        return final_bleu, all_other_metrics

    def _increment_save_counter(self):
        """
        å¢åŠ ä¿å­˜è®¡æ•°å™¨å¹¶åœ¨å¿…è¦æ—¶åˆ·æ–°ç¼“å­˜
        """
        self.SAVE_COUNTER += 1
        if self.SAVE_COUNTER > 2000:  # æ¯100æ¬¡ä¿å­˜
            self.flush_ec()
    
    def compute_advantages_for_episodes(self, trajectories, rewards, q_values):
        """ä¸ºepisodeè½¨è¿¹è®¡ç®—advantages"""
        returns = []
        advantages = []
        
        # æŒ‰episodeåˆ†ç»„å¤„ç†
        episode_starts = [0]
        for i, step in enumerate(trajectories):
            if step['is_terminal']:
                episode_starts.append(i + 1)
        
        for start_idx in range(len(episode_starts) - 1):
            episode_start = episode_starts[start_idx]
            episode_end = episode_starts[start_idx + 1]
            
            # è®¡ç®—è¿™ä¸ªepisodeçš„returns
            episode_rewards = rewards[episode_start:episode_end]
            episode_q_values = q_values[episode_start:episode_end]
            
            # ä»åå¾€å‰è®¡ç®—discounted returns
            episode_returns = []
            running_return = 0.0
            
            for r in reversed(episode_rewards):
                running_return = r + self.args.gamma * running_return
                episode_returns.append(running_return)
            
            episode_returns.reverse()
            
            # è®¡ç®—advantages
            episode_advantages = [ret - q_val for ret, q_val in zip(episode_returns, episode_q_values)]
            
            returns.extend(episode_returns)
            advantages.extend(episode_advantages)
        
        return torch.tensor(returns, device=self.device), torch.tensor(advantages, device=self.device)

    def collect_episode(self, crItems, batch_idx=0, test_flag="train", merged_data=None):
        """æ”¶é›†ä¸€ä¸ªå®Œæ•´çš„episode"""
        if merged_data is not None:
            merged_data = merged_data
        else:
            core_ids = ["PR-"+str(crItem["ghid"]) for crItem in crItems]
            merged_data = collect_episodes_merged(crItems, batch_idx,  self.rfDataTrans, self.predefined_args, self.device, core_ids, test_flag=test_flag)
            
        batch_data, global_node_to_idx = merged_data["batch_data"], merged_data["global_node_to_idx"]
        crItems, core_ids, node_neighbors = merged_data["crItems"], merged_data["core_ids"], merged_data["node_neighbors"]
        idx_to_global_node =  {v: k for k, v in global_node_to_idx.items()}

        current_nodes=[[index] for index in core_ids]
        num_graphs = len(core_ids)
        pre_bleu = [0.0]*num_graphs
        # 2. å¤„ç†æ¯ä¸ªå›¾çš„åŠ¨ä½œå’Œå¥–åŠ±
        trajectories = [[] for _ in range(num_graphs)]
        is_stop = [False] * num_graphs
        index = 0
        max_bleu_per_trajectry = []
        max_other_metrics_per_trajectry = []
        while not all(is_stop):
            pre_bleu = copy.deepcopy(pre_bleu)
            rewards = [None] * len(is_stop)
            states = [None] * len(is_stop)  # ä¿å­˜æ¯ä¸ªå›¾çš„çŠ¶æ€
            other_metrics = [{}]*num_graphs
            # 2. æ›´æ–°æ¯å›¾çš„ current_nodes å’Œ is_stop
            batch_data = batch_data.to(self.device)
            action, log_prob, entropy, selected_node, is_stop = self.actor.sample_action(batch_data, current_nodes, node_neighbors, maxRL=self.predefined_args.maxRL)
            for graph_idx in range(num_graphs):
                selected_node_idx = selected_node[graph_idx]
                current_nodes[graph_idx].append(selected_node_idx)
                if is_stop[graph_idx] and index!=0:
                    # å¦‚æœè¯¥å›¾å·²ç»åœæ­¢ï¼Œå¥–åŠ±å¢ç›Šä¸º0
                    reward = 0.000001
                else:
                    metrics = self.calculate_bleu_from_cache(crItems[graph_idx])
                    if metrics != False:  
                        cur_bleu = metrics["bleu"]
                        reward = cur_bleu - pre_bleu[graph_idx] + 0.000001
                        pre_bleu[graph_idx] = cur_bleu
                        other_metrics[graph_idx] = metrics
                    else:
                        reward = False
                if self.predefined_args.task_type == "msg":
                    state = self.rfDataTrans.tokenize(item=(crItems[graph_idx], self.rfDataTrans.tokenizer, self.predefined_args))
                elif self.predefined_args.task_type == "ref":
                    state = self.rfDataTrans.tokenize_for_ref(item=(crItems[graph_idx], self.rfDataTrans.tokenizer, self.predefined_args))
                else:
                    raise "Only msg and ref could be assigned to task_type in args"
                rewards[graph_idx] = reward
                states[graph_idx] = state
            if not all(rewards):
                incomplete_indices = [i for i, status in enumerate(rewards) if status==False]
                # æå–å¯¹åº”çš„æ•°æ®å’ŒçŠ¶æ€
                reeval_states = [states[i] for i in incomplete_indices]
                reeval_crItems = [crItems[i] for i in incomplete_indices]
                completed_bleu, ep_other_metrics = self.calculate_bleu_from_preTrainedModel(reeval_states, reeval_crItems)
                for i, missed_index in enumerate(incomplete_indices):
                    rewards[missed_index] = completed_bleu[i] - pre_bleu[missed_index]
                    pre_bleu[missed_index] = completed_bleu[i]
                    other_metrics[missed_index] = ep_other_metrics[i]
                
            # 3. è®°å½•è½¨è¿¹ï¼ˆä¸ºæ¯ä¸ªå›¾è®°å½•ï¼‰
            for graph_idx in range(num_graphs):
                # å°†current_nodesè½¬æ¢ä¸ºå…¨å±€èŠ‚ç‚¹ID
                node_ids = [idx_to_global_node[id_].split("node_")[-1] for id_ in current_nodes[graph_idx] if id_ in idx_to_global_node.keys()]
                trajectories[graph_idx].append({
                    'state': torch.tensor(states[graph_idx].source_ids, dtype=torch.float32),
                    'action': selected_node[graph_idx],
                    'log_prob': log_prob[graph_idx].to(self.device),
                    'entropy': entropy[graph_idx].to(self.device),
                    'reward': rewards[graph_idx],
                    'is_terminal': is_stop[graph_idx],
                    'node_ids': node_ids
                })
                # æ›´æ–°è¯¥å›¾çš„context
                crItems[graph_idx]["context"] = generate_context_from_nodeList(crItems[graph_idx]["graph_data"], node_ids)
            
            max_bleu_per_trajectry.append(pre_bleu)
            max_other_metrics_per_trajectry.append(other_metrics)
            # æ¸…æ˜¾å¡
            torch.cuda.empty_cache()
            index+=1
            # print(f"index:{index}")
        all_repo_ids = [item["repo_id"] for item in crItems]
        return trajectories, max_bleu_per_trajectry, max_other_metrics_per_trajectry, all_repo_ids
    
    def evaluate(self, dataloader, test_flag):
        """è¯„ä¼°æ¨¡å‹ï¼ˆå¤šå›¾å¤šstepç‰ˆæœ¬ï¼‰"""
        self.actor.eval()
        self.critic.eval()
        episode_other_metrics = []
        all_rewards = []  # æ¯ä¸ªepisodeçš„æ€»reward
        all_repo_ids = []
        all_selected_nodes = []
        with torch.no_grad():
            for batch_idx, batch_data in enumerate(dataloader):
                # è·‘åéæ”¶é›†episodeï¼Œæ‰¾æœ€å¤§å€¼
                all_max_bleu_per_trajectory = []
                all_other_metrics = []
                all_trajactories = []
                core_ids = ["PR-"+str(crItem["ghid"]) for crItem in batch_data]
                merged_data = collect_episodes_merged(batch_data, batch_idx,  self.rfDataTrans, self.predefined_args, self.device, core_ids, test_flag=test_flag)
                
                for run_idx in range(20):
                    # æ”¶é›†æ¯ä¸ªå›¾çš„episode
                    # print("run_idx:run_idx")
                    print("run_idx:{run_idx}")
                    trajactory, max_bleu_per_trajectory, other_metrics, episode_repo_ids = self.collect_episode(batch_data, batch_idx, test_flag=test_flag, merged_data=merged_data)
                    all_max_bleu_per_trajectory.extend(max_bleu_per_trajectory)
                    all_other_metrics.extend(other_metrics)
                    all_trajactories.extend(trajactory)
                    
                    # å°†10æ¬¡è¿è¡Œçš„ç»“æœåˆå¹¶
                    np_max_bleu = pad_trajectories_to_numpy(all_max_bleu_per_trajectory)
                    max_indices = np.argmax(np_max_bleu, axis=0)  # æ‰¾æ¯åˆ—çš„æœ€å¤§å€¼ç´¢å¼•
                    # é€‰æ‹©å¯¹åº”çš„æœ€ä½³ç»“æœ
                    selected_other_metrics = []
                    selected_context_nodeList = []
                    for trajectory_idx in range(len(max_indices)):
                        best_run_idx = max_indices[trajectory_idx]
                        selected_other_metrics.append(all_other_metrics[best_run_idx][trajectory_idx])
                        selected_context_nodeList.append(all_trajactories[trajectory_idx][best_run_idx%(self.predefined_args.maxRL+1)]["node_ids"])
                    
                    episode_other_metrics.extend(selected_other_metrics)
                    col_maxes = np.max(np_max_bleu, axis=0)  # æ¯åˆ—çš„æœ€å¤§å€¼
                    all_rewards.extend(col_maxes)
                    all_repo_ids.extend(episode_repo_ids)  # æ³¨æ„ï¼šè¿™é‡Œå¯èƒ½éœ€è¦è°ƒæ•´ï¼Œå› ä¸ºç°åœ¨æœ‰10æ¬¡è¿è¡Œ
                    all_selected_nodes.extend(selected_context_nodeList)
                
                    mean_reward = np.mean(all_rewards) if all_rewards else 0.0
                    print(f"run_idx:{run_idx}, mean_bleu:{mean_reward}")
        df = None
        if test_flag=="test":
            df = pd.DataFrame(episode_other_metrics)
            other_metrics_df = df.mean().to_dict()
            df['bleu'] = all_rewards
            df['repo_id'] = all_repo_ids
            df['selected_nodeLs'] = all_selected_nodes
            other_metrics_df["bleu"] = mean_reward
        self.actor.train()
        self.critic.train()
        return mean_reward, df

    def train_step(self, batch_data, batch_idx):
        """å•æ­¥è®­ç»ƒ - å¤„ç†å®Œæ•´episodes"""
        # batch_data å°±æ˜¯ä¸€ä¸ªlist
        trajectories_start_time = time.time()
        # 1. æ”¶é›†æ‰€æœ‰episodesçš„è½¨è¿¹
        all_trajectories, _, _, _ = self.collect_episode(batch_data, batch_idx)
        trajectories_end_time = time.time()
        logger.info(f"é•¿åº¦ä¸º{len(batch_data)}çš„ä¸€ä¸ªæ‰¹æ¬¡ï¼Œå…¶ç”Ÿæˆè¯¥æ‰¹æ¬¡çš„å…¨éƒ¨trajectoriesçš„æ—¶é•¿ä¸º{trajectories_end_time-trajectories_start_time}ç§’")
        
        # all_trajectories: List[List[Dict]]ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå›¾çš„episodeï¼ˆå¤šstepï¼‰
        # --- collect flattened lists and predicted values ---
        states = []
        actions = []
        log_probs = []
        rewards = []
        entropies = []
        is_terminals = []
        predicted_state_values = []
        state_values = []
        
        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†æ‰€æœ‰æ•°æ®
        all_step_states = []
        all_step_count = 0
        for episode in all_trajectories:
            for step in episode:
                states.append(step['state'])
                actions.append(step.get('action'))
                log_probs.append(step['log_prob'])
                rewards.append(float(step['reward']))
                entropies.append(step['entropy'])
                is_terminals.append(bool(step['is_terminal']))
                all_step_states.append(step['state'])
                if is_terminals!=True:
                    all_step_count+=1
        
        # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡è®¡ç®—critic values (è¿™æ˜¯å”¯ä¸€çš„ä¼˜åŒ–éƒ¨åˆ†)
        if all_step_states:
            batch_states = torch.stack(all_step_states).to(self.device)  # [N, 256]
            batch_v_vals = self.critic(batch_states).view(-1)            # [N]
            
            # è½¬æ¢å›åŸæ¥çš„æ ¼å¼
            for v_val in batch_v_vals:
                predicted_state_values.append(v_val.view(1))
                state_values.append(v_val.item())

        # stack tensors
        log_probs = torch.stack(log_probs)                # [N]
        entropies = torch.stack(entropies)                # [N]
        predicted_state_values = torch.cat(predicted_state_values, dim=0).to(self.device).float()  # [N]
        rewards_tensor = torch.tensor(rewards, dtype=torch.float32, device=self.device)            # [N]
        is_terminals = np.array(is_terminals, dtype=np.bool_)                                        # [N] (numpy for indexing)

        # --- GAE parameters ---
        gamma = getattr(self.args, 'gamma', 0.99)
        lam = getattr(self.args, 'gae_lambda', 0.95)

        # --- compute GAE per-episode (reverse) ---
        advantages_list = []
        returns_list = []
        idx = 0
        for episode in all_trajectories:
            ep_len = len(episode)
            if ep_len == 0:
                continue

            # slice episode tensors
            ep_rewards = rewards_tensor[idx: idx + ep_len]               # tensor on device
            ep_values = predicted_state_values[idx: idx + ep_len]        # tensor on device
            ep_terminal = is_terminals[idx: idx + ep_len]                # numpy bool array

            # prepare advantage tensor
            advantages = torch.zeros(ep_len, device=self.device)
            lastgaelam = 0.0

            # reversed GAE
            for t in range(ep_len - 1, -1, -1):
                # next_value = v_{t+1} if exists and not terminal at t (i.e., if this step is not terminal),
                # else 0. We treat ep_terminal[t] == True meaning this step is terminal (no bootstrap).
                if t == ep_len - 1:
                    next_value = 0.0
                    next_nonterminal = 0.0 if ep_terminal[t] else 1.0
                else:
                    # next_value = value[t+1] (bootstrap)
                    next_value = ep_values[t + 1].item()
                    next_nonterminal = 0.0 if ep_terminal[t] else 1.0

                delta = ep_rewards[t].item() + gamma * next_value * next_nonterminal - ep_values[t].item()
                lastgaelam = delta + gamma * lam * next_nonterminal * lastgaelam
                advantages[t] = lastgaelam

            # returns = advantages + values
            ep_returns = advantages + ep_values.detach()
            advantages_list.append(advantages)
            returns_list.append(ep_returns)

            idx += ep_len

        # concat episode results to single tensors (same order as flattened lists)
        if len(advantages_list) > 0:
            advantages = torch.cat(advantages_list, dim=0)   # [N]
            returns_tensor = torch.cat(returns_list, dim=0)  # [N]
        else:
            # fallback to zeros if no data
            advantages = torch.zeros(0, device=self.device)
            returns_tensor = torch.zeros(0, device=self.device)

        # --- advantage normalization (recommended) ---
        if advantages.numel() > 0:
            adv_mean = advantages.mean()
            adv_std = advantages.std(unbiased=False)
            advantages = (advantages - adv_mean) / (adv_std + 1e-8)
            # keep a copy for logging before detach if needed
        else:
            advantages = advantages

        # clamp advantages optionally (kept small clipping as safety)
        advantages = torch.clamp(advantages, -10, 10)

        # --- critic loss (fit V(s) to returns) ---
        # predicted_state_values is V(s) for each step
        critic_loss = F.smooth_l1_loss(predicted_state_values, returns_tensor)

        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.args.max_grad_norm)
        self.critic_optimizer.step()

        # --- actor loss (policy gradient) ---
        # note: minimize loss -> we put negative sign for policy gradient and subtract entropy bonus
        policy_loss = -(log_probs * advantages.detach()).mean()
        entropy_loss = entropies.mean()  # want to maximize entropy -> subtract in loss
        actor_loss = policy_loss - self.args.entropy_weight * entropy_loss

        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.args.max_grad_norm)
        self.actor_optimizer.step()

        # --- logging / returns ---
        total_steps = all_step_count
        avg_reward = float(torch.mean(torch.tensor([s['reward'] for ep in all_trajectories for s in ep], device=self.device))) if total_steps > 0 else 0.0
        avg_advantage = float(advantages.mean().item()) if advantages.numel() > 0 else 0.0
        trajectory_sums = [sum(s['reward'] for s in ep) for ep in all_trajectories]
        avg_sum_reward = float(torch.mean(torch.tensor(trajectory_sums, device=self.device)))
        return {
            'actor_loss': actor_loss.item(),
            'critic_loss': critic_loss.item(),
            'avg_reward': avg_reward,
            'avg_advantage': avg_advantage,
            'avg_sum_reward': avg_sum_reward,
            'avg_episode_length': total_steps / max(1, len(all_trajectories))
        }

    def train_epoch(self, dataloader, epoch_num=None, visualizer: Optional['TrainingVisualizer'] = None):
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆæ”¯æŒå¤šå›¾å¤šstepçš„batchï¼‰- é›†æˆå¯è§†åŒ–åŠŸèƒ½"""
        self.actor.train()
        self.critic.train()
        
        epoch_stats = {
            'actor_losses': [],
            'critic_losses': [],
            'rewards': [],
            'advantages': [],
            'episode_lengths': [],
            'avg_sum_reward': []
        }
        start_time = time.time()
        
        for batch_idx, batch_data in enumerate(dataloader):
            batch_start_time = time.time()
            # # è®­ç»ƒä¸€ä¸ªbatchï¼ˆå¤šå›¾å¤šstepï¼‰
            batch_idx += self.predefined_args.baseBatchIndex
            stats = self.train_step(batch_data, batch_idx)
            
            # ç´¯ç§¯ç»Ÿè®¡
            epoch_stats['actor_losses'].append(stats['actor_loss'])
            epoch_stats['critic_losses'].append(stats['critic_loss'])
            epoch_stats['rewards'].append(stats['avg_reward'])
            epoch_stats['advantages'].append(stats['avg_advantage'])
            epoch_stats['episode_lengths'].append(stats['avg_episode_length'])
            epoch_stats['avg_sum_reward'].append(stats["avg_sum_reward"])
            
            batch_time = time.time() - batch_start_time
            
            # å¯è§†åŒ–è®°å½•
            if visualizer:
                batch_metrics = {
                    'actor_loss': stats['actor_loss'],
                    'critic_loss': stats['critic_loss'],
                    'avg_reward': stats['avg_reward'],
                    'avg_advantage': stats['avg_advantage'],
                    'avg_episode_length': stats['avg_episode_length'],
                    'avg_sum_reward':np.mean(epoch_stats['avg_sum_reward']),
                    'batch_time': batch_time
                }
                visualizer.log_batch_metrics(batch_metrics, batch_idx)
                
                if batch_idx % self.args.log_interval == 0:
                    visualizer.log_learning_rates({
                        'Actor': self.actor_optimizer,
                        'Critic': self.critic_optimizer
                    })
        # Epochç»Ÿè®¡
        avg_stats = {
            'avg_actor_loss': np.mean(epoch_stats['actor_losses']),
            'avg_critic_loss': np.mean(epoch_stats['critic_losses']),
            'avg_reward': np.mean(epoch_stats['rewards']),
            'all_return': np.sum(epoch_stats['rewards']),
            'avg_advantage': np.mean(epoch_stats['advantages']),
            'avg_episode_length': np.mean(epoch_stats['episode_lengths']),
            'std_reward': np.std(epoch_stats['rewards']),
            'avg_sum_reward':np.mean(epoch_stats['avg_sum_reward']),
            'epoch_time': time.time() - start_time
        }
        
        # å¯è§†åŒ–Epochæ€»ç»“
        if visualizer:
            visualizer.log_epoch_summary(epoch_num or 0, avg_stats)
        
        return avg_stats

    # è§„åˆ’trainä¸testçš„éƒ¨åˆ†
    def run(self, train_dataloader, val_dataloader=None, num_epochs=1000):
        """è¿è¡Œè®­ç»ƒï¼ˆå¤šå›¾å¤šstepï¼‰"""
        logger.info("Starting Simplified RL Training")
        
        best_reward = -float('inf')
        visualizer = TrainingVisualizer(experiment_name=self.predefined_args.rlModel)
        for epoch in range(num_epochs):
            # è®­ç»ƒ
            train_stats = self.train_epoch(train_dataloader,  epoch_num=epoch, visualizer=visualizer)
            # ä¸€ä¸ªepochå°±æ˜¯ä¸€è½®è®­ç»ƒï¼ˆå¤šä¸ªå›¾ï¼‰
            self.episode_rewards.append(train_stats['avg_reward'])
            self.actor_losses.append(train_stats['avg_actor_loss'])
            self.critic_losses.append(train_stats['avg_critic_loss'])
            
            if ((epoch % 10 == 0 and epoch!=0 and epoch < 80) or (epoch % 1 == 0 and epoch > 80)) and val_dataloader is not None:
                val_reward, _ = self.evaluate(val_dataloader, test_flag="valid")
                
                # è®°å½•åˆ° TensorBoard
                re = {
                    "Validation/Reward": val_reward,
                    "Train/Reward": train_stats['avg_reward'],
                    "Train/Actor_Loss": train_stats['avg_actor_loss'],
                    "Train/Critic_Loss": train_stats['avg_critic_loss']
                }
                visualizer.log_scalar("Validation/Reward", val_reward, step=epoch)
                visualizer.log_scalars({
                    "Validation/Reward": val_reward,
                    "Train/Reward": train_stats['avg_reward'],
                    "Train/Actor_Loss": train_stats['avg_actor_loss'],
                    "Train/Critic_Loss": train_stats['avg_critic_loss']
                }, step=epoch)
                print(re)
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_reward > best_reward:
                    best_reward = val_reward
                    self.save_models(epoch, is_best=True)
                else:
                    self.save_models(epoch, is_best=False)

    def save_models(self, epoch, is_best=False):
        """ä¿å­˜æ¨¡å‹"""
        suffix = '_best' if is_best else f'_epoch_{epoch}'
        
        torch.save({
            'actor_state_dict': self.actor.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'epoch': epoch,
            'episode_rewards': self.episode_rewards,
            'actor_losses': self.actor_losses,
            'critic_losses': self.critic_losses
        }, f'{self.args.output_dir}/rl_model{suffix}.pt')
        logger.info(f"Model saved: rl_model{suffix}.pt")

    def load_models(self, model_path, load_optimizer=True):
        """åŠ è½½æ¨¡å‹"""
        try:
            checkpoint = torch.load(model_path, map_location=self.device)
            
            # åŠ è½½æ¨¡å‹å‚æ•°
            self.actor.load_state_dict(checkpoint['actor_state_dict'])
            self.critic.load_state_dict(checkpoint['critic_state_dict'])
            
            # å¯é€‰æ‹©æ˜¯å¦åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            if load_optimizer:
                self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
                self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
            
            # åŠ è½½è®­ç»ƒå†å²ä¿¡æ¯
            epoch = checkpoint.get('epoch', 0)
            self.episode_rewards = checkpoint.get('episode_rewards', [])
            self.actor_losses = checkpoint.get('actor_losses', [])
            self.critic_losses = checkpoint.get('critic_losses', [])
            
            logger.info(f"Model loaded successfully from {model_path}")
            logger.info(f"Resumed from epoch {epoch}")
            
            return epoch
            
        except FileNotFoundError:
            logger.error(f"Model file not found: {model_path}")
            raise
        except KeyError as e:
            logger.error(f"Missing key in checkpoint: {e}")
            raise
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            raise